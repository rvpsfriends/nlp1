{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:S.vidhya\n",
    "Regno:235229146\n",
    "Lab1: Understanding large text files\n",
    "ex2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the file: O. Henry's The Gift of the\n",
    "Magi (gift-of-magi.txt).\n",
    "2. Write a Python script to print out the\n",
    "following:\n",
    "1.   \n",
    "How many word tokens there\n",
    "are\n",
    "2.   \n",
    "How many word types there\n",
    "are, (word types are a unique set of words)\n",
    "3.   \n",
    "Top 20 most frequent words\n",
    "and their counts\n",
    "4.   \n",
    "Words that are at least 10 characters\n",
    "long and their counts\n",
    "5.   \n",
    "10+ characters-long words\n",
    "that occur at least twice, sorted from most frequent to least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\1mscdsa46/nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data', 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\1mscdsa46\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1mscdsa46\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gift-of-magi.txt\",\"r\")as file:\n",
    "    text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of word tokens: 2466\n",
      "2. Number of word types: 767\n",
      "3. Top 20 most frequent words and their counts:\n",
      "   .: 141\n",
      "   the: 118\n",
      "   ,: 104\n",
      "   and: 83\n",
      "   a: 68\n",
      "   of: 52\n",
      "   to: 41\n",
      "   it: 39\n",
      "   she: 33\n",
      "   ``: 30\n",
      "   '': 30\n",
      "   was: 27\n",
      "   her: 27\n",
      "   jim: 26\n",
      "   in: 25\n",
      "   had: 23\n",
      "   you: 23\n",
      "   for: 22\n",
      "   that: 21\n",
      "   with: 21\n",
      "4. Words that are at least 10 characters long and their counts:\n",
      "   eighty-seven: 3\n",
      "   bulldozing: 1\n",
      "   imputation: 1\n",
      "   instigates: 1\n",
      "   reflection: 4\n",
      "   predominating: 1\n",
      "   description: 2\n",
      "   mendicancy: 1\n",
      "   letter-box: 1\n",
      "   appertaining: 1\n",
      "   dillingham: 6\n",
      "   prosperity: 1\n",
      "   contracting: 1\n",
      "   unassuming: 1\n",
      "   introduced: 1\n",
      "   calculated: 1\n",
      "   pier-glass: 2\n",
      "   longitudinal: 1\n",
      "   conception: 1\n",
      "   brilliantly: 1\n",
      "   possessions: 1\n",
      "   grandfather: 1\n",
      "   depreciate: 1\n",
      "   ransacking: 1\n",
      "   proclaiming: 1\n",
      "   meretricious: 1\n",
      "   ornamentation: 1\n",
      "   twenty-one: 1\n",
      "   intoxication: 1\n",
      "   generosity: 1\n",
      "   tremendous: 1\n",
      "   close-lying: 1\n",
      "   wonderfully: 2\n",
      "   critically: 1\n",
      "   frying-pan: 1\n",
      "   twenty-two: 1\n",
      "   expression: 2\n",
      "   disapproval: 1\n",
      "   sentiments: 1\n",
      "   laboriously: 1\n",
      "   inconsequential: 1\n",
      "   difference: 1\n",
      "   mathematician: 1\n",
      "   illuminated: 1\n",
      "   hysterical: 1\n",
      "   necessitating: 1\n",
      "   employment: 1\n",
      "   comforting: 1\n",
      "   worshipped: 1\n",
      "   tortoise-shell: 1\n",
      "   possession: 1\n",
      "   adornments: 1\n",
      "   duplication: 1\n",
      "   uneventful: 1\n",
      "   sacrificed: 1\n",
      "   everywhere: 1\n",
      "5. 10+ characters-long words occurring at least twice, sorted by frequency:\n",
      "   dillingham: 6\n",
      "   reflection: 4\n",
      "   eighty-seven: 3\n",
      "   description: 2\n",
      "   pier-glass: 2\n",
      "   wonderfully: 2\n",
      "   expression: 2\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "words = word_tokenize(text.lower())\n",
    "# Counting word tokens\n",
    "num_tokens = len(words)\n",
    "\n",
    "# Counting word types\n",
    "num_types = len(set(words))\n",
    "\n",
    "# Calculating word frequency\n",
    "word_freq = FreqDist(words)\n",
    "\n",
    "# Top 20 most frequent words and their counts\n",
    "top_20 = word_freq.most_common(20)\n",
    "\n",
    "# Words at least 10 characters long and their counts\n",
    "long_words = {word: count for word, count in word_freq.items() if len(word) >= 10}\n",
    "\n",
    "# 10+ characters-long words occurring at least twice, sorted by frequency\n",
    "long_words_twice = [word for word, count in word_freq.items() if len(word) >= 10 and count >= 2]\n",
    "long_words_twice_sorted = sorted(long_words_twice, key=lambda x: word_freq[x], reverse=True)\n",
    "\n",
    "# Displaying results\n",
    "print(\"1. Number of word tokens:\", num_tokens)\n",
    "print(\"2. Number of word types:\", num_types)\n",
    "print(\"3. Top 20 most frequent words and their counts:\")\n",
    "for word, count in top_20:\n",
    "    print(f\"   {word}: {count}\")\n",
    "print(\"4. Words that are at least 10 characters long and their counts:\")\n",
    "for word, count in long_words.items():\n",
    "    print(f\"   {word}: {count}\")\n",
    "print(\"5. 10+ characters-long words occurring at least twice, sorted by frequency:\")\n",
    "for word in long_words_twice_sorted:\n",
    "    print(f\"   {word}: {word_freq[word]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
